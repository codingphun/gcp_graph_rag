{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2c901-db3b-4138-9a53-33f6a61031ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mRmcE_M3h9-N",
   "metadata": {
    "id": "mRmcE_M3h9-N"
   },
   "source": [
    "\n",
    "# GraphRAG on Google Cloud (Part Two)\n",
    "## Vertex AI Agent Framework\n",
    "\n",
    "\n",
    "\n",
    "|Author(s) | [Smitha Venkat](https://github.com/smitha-google), [Tristan Li](https://github.com/codingphun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe194ad-2ddf-4340-8f4f-5eee078742f8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the [first part](https://github.com/codingphun/gcp_graph_rag/blob/main/graph_rag_spanner.ipynb) of the tutorial, GraphRAG on Google Cloud, you learned how to use popular open-source LangChain framework. In this second part tutorial, you will learn how to use Vertex AI Agent Framework. The goal Vertex AI Agent Framework is to deliver a industryâ€™s leading, enterprise ready agent platform that can power real world agentic use cases, along with state-of-the-art exemplar solutions built on top of the Google Cloud platform.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this tutorial, you will leverage the graph and vector database created in part one tutorial and build an agent with the Vertex AI Agent Framework. You will also learn how to use Function Calling in the agent to look up information in knowledge graph stored in Spanner. Finally, you will learn how to leverage Gemini Google Search grounding to look up additional information that may not exist in the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a123e8-8247-42ce-b05c-440223741c15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "1. Make sure to [sign up](https://docs.google.com/forms/d/1uTPDw0TWrj7EZRXE-8UTl4mpsTWelBjDu3H-0FFWlXg/viewform?edit_requested=true) for the Vertex AI Agent Framework first to receive the SDK needed for this tutorial.\n",
    "1. Make sure to run the [first part](https://github.com/codingphun/gcp_graph_rag/blob/main/graph_rag_spanner.ipynb) of the tutorial to create the knowledge graph, embedding vectors in the Spanner database.\n",
    "1. In the Google Cloud console, on the project selector page, select or [create a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).\n",
    "1. [Make sure that billing is enabled for your Google Cloud project](https://cloud.google.com/billing/docs/how-to/verify-billing-enabled#console).\n",
    "\n",
    "\n",
    "### Required roles\n",
    "\n",
    "To get the permissions that you need to complete the tutorial, ask your administrator to grant you the [Owner](https://cloud.google.com/iam/docs/understanding-roles#owner) (`roles/owner`) IAM role on your project. \n",
    "For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75580252-1135-4724-a807-ca9c73aaec8c",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6a1a6-2214-490e-89ca-ce599347df84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sQXbC9DZlzt3JG2J1rXqA3rf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sQXbC9DZlzt3JG2J1rXqA3rf",
    "outputId": "3f7c6bf7-4228-43d4-937c-5a62965a9f55",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the genai agent whl file in the current instance.\n",
    "\n",
    "!pip3 install google_genai_agents-0.0.2.dev*.whl --quiet --ignore-requires-python\n",
    "\n",
    "!pip install google-cloud-spanner==2.0.0 --quiet\n",
    "!pip install --upgrade google-cloud-aiplatform --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B1f_W8Ux4uqL",
   "metadata": {
    "id": "B1f_W8Ux4uqL"
   },
   "source": [
    "### Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc8754-a6e9-4856-bf2d-592371f36def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9bf6b-3cd4-4999-9fee-79681bcb1931",
   "metadata": {},
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f701216-7930-48c4-9ad7-384d706a0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()\n",
    "print(sys.version)\n",
    "# If using local jupyter instance, uncomment and run:\n",
    "# !gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6485b-4782-4d2f-8f35-57a34a2fdf50",
   "metadata": {},
   "source": [
    "### CHANGE the following settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rEmUCRbGqHi0",
   "metadata": {
    "id": "rEmUCRbGqHi0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = \"\" \n",
    "LOCATION = \"us-central1\"\n",
    "SPANNER_INSTANCE_ID='graphrag-instance'\n",
    "SPANNER_DATABASE_ID='graphrag'\n",
    "MODEL_NAME='gemini-1.5-flash-002'\n",
    "EMBEDDING_MODEL_NAME='text-embedding-004'\n",
    "\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"1\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GOOGLE_CLOUD_PROJECT\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6f7b0-1596-448f-b6d7-35bccc9c696f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "PK72v34ShnC7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PK72v34ShnC7",
    "outputId": "f5699280-962f-47b3-cc05-70fc27c192f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import the packages\n",
    "\n",
    "from agents.tools.base_tool import ToolContext\n",
    "from google.cloud import spanner\n",
    "from agents import Runner, Agent\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "from vertexai.generative_models import (\n",
    "    Tool,\n",
    "    grounding,\n",
    "    GenerativeModel,\n",
    "    GenerationConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310f560-df53-4c2f-9bf7-d7b14f7b8bd3",
   "metadata": {},
   "source": [
    "### Create helper functions for function calling\n",
    "\n",
    "We will create a couple python functions for function calling for the agent. The first function is to query the Spanner database for database through semantic search and graph traversal. The second function is to leverage Gemini Google Seach grounding to look for additional information that may not exist in the Spanner knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "UfGjb37FiTVE",
   "metadata": {
    "id": "UfGjb37FiTVE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "def get_embedding(text, task_type, model):\n",
    "  \"\"\"Gets embeddings for the input text.\n",
    "\n",
    "  Args:\n",
    "    response: text, task_type, model\n",
    "\n",
    "  Returns:\n",
    "    The values from the embedding model\n",
    "  \"\"\"\n",
    "  try:\n",
    "    text_embedding_input = TextEmbeddingInput(task_type=task_type, text=text)\n",
    "    embeddings = model.get_embeddings([text_embedding_input])\n",
    "    return embeddings[0].values\n",
    "  except:\n",
    "    return []\n",
    "\n",
    "def ask_graph(query:str) -> list[dict]:\n",
    "  \"\"\"Asks the pre-built graph database. We are using Cloud Spanner as our Graph Database for this example.\n",
    "  Refer https://github.com/codingphun/gcp_graph_rag/blob/main/graph_rag_spanner.ipynb on how to build your Graph Database.\n",
    "\n",
    "  Args:\n",
    "    query: The question to ask the graph database.\n",
    "\n",
    "  Returns:\n",
    "    Output from the graph database\n",
    "  \"\"\"\n",
    "  # Set the Spanner Database credentials\n",
    "  spanner_client = spanner.Client(GOOGLE_CLOUD_PROJECT)\n",
    "  instance = spanner_client.instance(SPANNER_INSTANCE_ID)\n",
    "  database = instance.database(SPANNER_DATABASE_ID)\n",
    "  TASK_TYPE = \"QUESTION_ANSWERING\"\n",
    "  EMBEDDING_MODEL = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "  # Get the embeddings for the user query.\n",
    "  q_emb = get_embedding(query, TASK_TYPE, EMBEDDING_MODEL)\n",
    "\n",
    "  # Now the graph nodes, relationship and embedding vectors are stored in the Spanner database.\n",
    "  kgnodename=''\n",
    "  with database.snapshot() as snapshot:\n",
    "    results = snapshot.execute_sql(\n",
    "      \"\"\"SELECT DocId, NAME, Doc FROM KgNode ORDER BY COSINE_DISTANCE(DocEmbedding, @q_emb) limit 1\"\"\",\n",
    "      params={\"q_emb\": q_emb},\n",
    "      param_types={\"q_emb\": spanner.param_types.Array(spanner.param_types.FLOAT64)}  # Adjust FLOAT64 if needed\n",
    "      )\n",
    "    for row in results:\n",
    "      kgnodename=str(row[1])\n",
    "\n",
    "    # Traverse the Knowledge Graph after you get the node to uncover the relationships and connections.\n",
    "    tables = [\"NATIONALITY\", \"WORKED_AT\", \"INVESTMENT\", \"INFLUENCED_BY\"]\n",
    "    a = []\n",
    "    for table in tables:\n",
    "       with database.snapshot() as snapshot:\n",
    "        if table in (\"INFLUENCED_BY\", \"LOCATED_IN\"):\n",
    "          column_name = \"P1_Name\"\n",
    "        else:\n",
    "          column_name = \"P_Name\"\n",
    "\n",
    "        results = snapshot.execute_sql(\n",
    "            f\"\"\"\n",
    "            SELECT {column_name} AS Name, *\n",
    "            FROM {table}\n",
    "            WHERE {column_name} = '{kgnodename}'\n",
    "            \"\"\"\n",
    "        )\n",
    "        for row in results:\n",
    "            # Dynamically determine the index of the relevant column\n",
    "            name_index = row.index(kgnodename)\n",
    "            # Construct the output string using f-string\n",
    "            output_string = f\"{row[name_index]} {table} {row[2]}\"\n",
    "            print(output_string)\n",
    "  return f\"\"\"[{output_string}]\"\"\"\n",
    "\n",
    "def ask_gemini(question: str) -> str:\n",
    "  \"\"\"Ask Gemini the question grounded with Google Search.\n",
    "\n",
    "  Args:\n",
    "    question: The question to ask Gemini.\n",
    "\n",
    "  Returns:\n",
    "    The answer from Gemini models grounded with Google Search.\n",
    "  \"\"\"\n",
    "  model = GenerativeModel(MODEL_NAME)\n",
    "  # Use Google Search for grounding\n",
    "  tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())\n",
    "  prompt = f\"\"\"Please answer the question:{question}\n",
    "  Use markdown to structure your answer if it makes sense. Be comprehensive in your answer.\n",
    "  \"\"\"\n",
    "  response = model.generate_content(\n",
    "      prompt,\n",
    "      tools=[tool],\n",
    "      generation_config=GenerationConfig(\n",
    "          temperature=0.0,\n",
    "      ),\n",
    "  )\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d97ea-2d03-494f-8247-8fbf457ab676",
   "metadata": {},
   "source": [
    "### Create an Agent\n",
    "\n",
    "We will create a Vertex AI Agent and give some high-level instructions for the agent on how to interact with the users. Note the function calling directions in the instruction section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "UkQ8hYM8k2Gh",
   "metadata": {
    "id": "UkQ8hYM8k2Gh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the agent using the agent framework\n",
    "root_agent = Agent(\n",
    "    model=MODEL_NAME,\n",
    "    name='root_agent',\n",
    "    greeting_prompt = 'Tell the user about yourself and ask for the query.',\n",
    "    instruction = \"\"\" - You are a helpful information retrieval agent that can answer user's query from the knowledge graph\n",
    "                      and do a broader search if you cant find answer in the graph database.\n",
    "                    - After you get the user query, always check the graph database first.\n",
    "                    - If the query can be answered from the graph, then call the ask_graph tool.\n",
    "                    - If you are not able to find the answer in the graph, ask the user if they would like to do a broader search.\n",
    "                    - If the user says yes, then call the ask_gemini tool.\n",
    "                    - If the user says no, then ask them if there is anything else they would like to know.\n",
    "                    - Always be courteous and dont assume anything.\n",
    "                    - If you dont know an answer, please say I dont know the answer.\n",
    "                \"\"\",\n",
    "    flow='auto',\n",
    "    tools=[ask_graph, ask_gemini],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97765535-b728-4535-b3f9-60e68e6675fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will create a session for the agent to run in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1pkYeLqbVgpf",
   "metadata": {
    "id": "1pkYeLqbVgpf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Session Management\n",
    "from agents.sessions.in_memory_session_service import InMemorySessionService\n",
    "from agents.artifacts.in_memory_artifact_service import InMemoryArtifactService\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "session_service = InMemorySessionService()\n",
    "artifact_service = InMemoryArtifactService()\n",
    "runner = Runner(root_agent, artifact_service, session_service)\n",
    "session = session_service.create()\n",
    "\n",
    "def run_prompt(new_message: str):\n",
    "    content = types.Content(role='user', parts=[types.Part.from_text(text=new_message)])\n",
    "    print (content)\n",
    "    for event in runner.run(\n",
    "      session=session,\n",
    "      new_message=content,\n",
    "    ):\n",
    "        if event.content:\n",
    "          print(event.content.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cdfe5-fe35-4fdc-868e-ce205216820b",
   "metadata": {},
   "source": [
    "Let's test out the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "AzOGJFuJVi6A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzOGJFuJVi6A",
    "outputId": "d4c9c976-ed56-4f0a-a272-1fac52b0994f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Who influenced Larry Page')] role='user'\n",
      "{'parts': [{'function_call': {'args': {'query': 'Who influenced Larry Page?'}, 'name': 'ask_graph'}}], 'role': 'model'}\n",
      "Lawrence Edward Page WORKED_AT Alphabet Inc.\n",
      "Lawrence Edward Page WORKED_AT Google\n",
      "Lawrence Edward Page INVESTMENT Kitty Hawk\n",
      "Lawrence Edward Page INVESTMENT Opener\n",
      "Lawrence Edward Page INFLUENCED_BY Maria Montessori\n",
      "{'parts': [{'function_response': {'name': 'ask_graph', 'response': {'result': '[Lawrence Edward Page INFLUENCED_BY Maria Montessori]'}}}], 'role': 'user'}\n",
      "{'parts': [{'text': 'Based on the graph database, Maria Montessori influenced Larry Page.\\n'}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "# Test your Agent\n",
    "run_prompt('Who influenced Larry Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17aa51-8cb8-467e-ab6c-fb23738b1b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Google Searching Grounding\n",
    "\n",
    "Currently, the agent retrieves answers from the Spanner-backed knowledge graph.  For queries beyond the knowledge graph's scope, we can augement it with Google Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "FfdqxTfsV1Xg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfdqxTfsV1Xg",
    "outputId": "8efd38d9-2543-4c6d-ef1f-e59313bdf17f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='What is deep seek')] role='user'\n",
      "{'parts': [{'function_call': {'args': {'query': 'What is deep seek?'}, 'name': 'ask_graph'}}], 'role': 'model'}\n",
      "Lawrence Edward Page WORKED_AT Alphabet Inc.\n",
      "Lawrence Edward Page WORKED_AT Google\n",
      "Lawrence Edward Page INVESTMENT Kitty Hawk\n",
      "Lawrence Edward Page INVESTMENT Opener\n",
      "Lawrence Edward Page INFLUENCED_BY Maria Montessori\n",
      "{'parts': [{'function_response': {'name': 'ask_graph', 'response': {'result': '[Lawrence Edward Page INFLUENCED_BY Maria Montessori]'}}}], 'role': 'user'}\n",
      "{'parts': [{'text': 'I am sorry, I could not find the answer to your question in the graph database. Would you like me to perform a broader search using another tool?\\n'}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "# Calling the agent with another question that cannot be answered from knowledge graph\n",
    "run_prompt('What is deep seek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aBWTykP2V4IX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBWTykP2V4IX",
    "outputId": "ca0a57b9-298f-4248-bf1f-1f8779e9654c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Yes')] role='user'\n",
      "{'parts': [{'function_call': {'args': {'question': 'What is deep seek?'}, 'name': 'ask_gemini'}}], 'role': 'model'}\n",
      "{'parts': [{'function_response': {'name': 'ask_gemini', 'response': {'result': 'DeepSeek is a Chinese artificial intelligence (AI) company founded in May 2023 by Liang Wenfeng, who also co-founded the Chinese quantitative hedge fund High-Flyer, which owns DeepSeek.  The company is based in Hangzhou, Zhejiang, China.\\n\\nDeepSeek\\'s primary focus is the development of open-source large language models (LLMs).  Their flagship model, DeepSeek-R1, is notable for achieving performance comparable to models like OpenAI\\'s GPT-4, but at a significantly lower cost (reportedly $6 million versus $100 million for GPT-4) and using considerably less computing power (one-tenth).  This cost-effectiveness is particularly noteworthy given the US sanctions on China regarding Nvidia chips, which were intended to hinder the development of advanced AI systems.\\n\\nDeepSeek\\'s impact has been substantial.  By January 27th, 2025, their DeepSeek-R1-based chatbot app had become the most downloaded free app on the US iOS App Store, surpassing ChatGPT. This success led to an 18% drop in Nvidia\\'s share price.  Analysts describe DeepSeek\\'s achievements as \"upending AI\" and a significant development in the emerging global AI space race.  While DeepSeek-R1 is their most prominent model, they also have other AI models and technologies under development, including image generation capabilities.  The company employs fewer than 200 people.\\n'}}}], 'role': 'user'}\n",
      "{'parts': [{'text': \"DeepSeek is a Chinese artificial intelligence company founded in May 2023.  Their flagship model, DeepSeek-R1, is a large language model (LLM) that rivals the performance of models like OpenAI's GPT-4, but at a significantly lower cost and using less computing power.  By January 27th, 2025, their DeepSeek-R1-based chatbot app was the most downloaded free app on the US iOS App Store.\\n\"}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "# Answer yes to ask your agent to perform a Google Search\n",
    "run_prompt(\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b6593-d41a-44ab-a3b8-f93cb09cdd5a",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "The agent framework provides a robust list of each reasoning step taken by the agent which makes troubleshooting easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3392c-f98d-4217-b642-097c92369db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List out active sessions\n",
    "session_service.list_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98115b34-eac4-4b86-9fc8-95a9ccfadcab",
   "metadata": {},
   "source": [
    "Make sure you replace the \"session_id\" placeholder below from the above step, if there are multiple sessions returned, use the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3zBmomI7Vtzu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zBmomI7Vtzu",
    "outputId": "a915e1a8-8df6-4d09-9f89-de2106dc9b32",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'invocation_id': 'vMW81019',\n",
       "  'author': 'user',\n",
       "  'content': {'parts': [{'video_metadata': None,\n",
       "     'thought': None,\n",
       "     'code_execution_result': None,\n",
       "     'executable_code': None,\n",
       "     'file_data': None,\n",
       "     'function_call': None,\n",
       "     'function_response': None,\n",
       "     'inline_data': None,\n",
       "     'text': 'Who influenced Larry Page'}],\n",
       "   'role': 'user'},\n",
       "  'actions': {'skip_summarization': None,\n",
       "   'state_delta': None,\n",
       "   'artifact_delta': None,\n",
       "   'pending': None,\n",
       "   'transfer_to_agent': None,\n",
       "   'escalate': None},\n",
       "  'is_greeting': None,\n",
       "  'function_call_event_id': None,\n",
       "  'partial': None,\n",
       "  'code_execution_event_type': None,\n",
       "  'id': 'UhNTqv1C',\n",
       "  'timestamp': 1738337945.694343},\n",
       " {'invocation_id': 'vMW81019',\n",
       "  'author': 'root_agent',\n",
       "  'content': {'parts': [{'video_metadata': None,\n",
       "     'thought': None,\n",
       "     'code_execution_result': None,\n",
       "     'executable_code': None,\n",
       "     'file_data': None,\n",
       "     'function_call': {'id': None,\n",
       "      'args': {'query': 'Who influenced Larry Page?'},\n",
       "      'name': 'ask_graph'},\n",
       "     'function_response': None,\n",
       "     'inline_data': None,\n",
       "     'text': None}],\n",
       "   'role': 'model'},\n",
       "  'actions': {'skip_summarization': None,\n",
       "   'state_delta': None,\n",
       "   'artifact_delta': None,\n",
       "   'pending': None,\n",
       "   'transfer_to_agent': None,\n",
       "   'escalate': None},\n",
       "  'is_greeting': None,\n",
       "  'function_call_event_id': None,\n",
       "  'partial': None,\n",
       "  'code_execution_event_type': None,\n",
       "  'id': 'Q9Z6lIBN',\n",
       "  'timestamp': 1738337946.060659},\n",
       " {'invocation_id': 'vMW81019',\n",
       "  'author': 'root_agent',\n",
       "  'content': {'parts': [{'video_metadata': None,\n",
       "     'thought': None,\n",
       "     'code_execution_result': None,\n",
       "     'executable_code': None,\n",
       "     'file_data': None,\n",
       "     'function_call': None,\n",
       "     'function_response': {'id': None,\n",
       "      'name': 'ask_graph',\n",
       "      'response': {'result': '[Lawrence Edward Page INFLUENCED_BY Maria Montessori]'}},\n",
       "     'inline_data': None,\n",
       "     'text': None}],\n",
       "   'role': 'user'},\n",
       "  'actions': {'skip_summarization': None,\n",
       "   'state_delta': None,\n",
       "   'artifact_delta': None,\n",
       "   'pending': None,\n",
       "   'transfer_to_agent': None,\n",
       "   'escalate': None},\n",
       "  'is_greeting': None,\n",
       "  'function_call_event_id': 'Q9Z6lIBN',\n",
       "  'partial': None,\n",
       "  'code_execution_event_type': None,\n",
       "  'id': '7clSOHLE',\n",
       "  'timestamp': 1738337946.664095},\n",
       " {'invocation_id': 'vMW81019',\n",
       "  'author': 'root_agent',\n",
       "  'content': {'parts': [{'video_metadata': None,\n",
       "     'thought': None,\n",
       "     'code_execution_result': None,\n",
       "     'executable_code': None,\n",
       "     'file_data': None,\n",
       "     'function_call': None,\n",
       "     'function_response': None,\n",
       "     'inline_data': None,\n",
       "     'text': 'Based on the graph database, Maria Montessori influenced Larry Page.\\n'}],\n",
       "   'role': 'model'},\n",
       "  'actions': {'skip_summarization': None,\n",
       "   'state_delta': None,\n",
       "   'artifact_delta': None,\n",
       "   'pending': None,\n",
       "   'transfer_to_agent': None,\n",
       "   'escalate': None},\n",
       "  'is_greeting': None,\n",
       "  'function_call_event_id': None,\n",
       "  'partial': None,\n",
       "  'code_execution_event_type': None,\n",
       "  'id': 'hwWPrDH6',\n",
       "  'timestamp': 1738337946.97905}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_service.get(\"8ce93f23-1c76-4887-a0a2-5b5fc046bf60\").model_dump()[\"events\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f62b01-d1e2-49d2-a81c-e4ca0ab04fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "*   Delete the Spanner instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5403bc5-00eb-4e0b-b847-9c9d5287ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud spanner instances delete {SPANNER_INSTANCE_ID} --quiet"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AgentFramework-GraphAgent",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
